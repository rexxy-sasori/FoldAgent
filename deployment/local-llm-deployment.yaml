apiVersion: apps/v1
kind: Deployment
metadata:
  name: local-llm
  namespace: liuyunxin
  labels:
    app: local-llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: local-llm
  template:
    metadata:
      labels:
        app: local-llm
    spec:
      containers:
      - name: nvidia-dcgm-exporter
        image: harbor.xa.xshixun.com:7443/hanfeigeng/docker.io/nvidia/dcgm-exporter:4.4.2-4.7.1-ubuntu22.04-linux-amd64
        ports:
        - containerPort: 9400
        securityContext:
          runAsUser: 0
          capabilities:
            add:
            - SYS_ADMIN
        resources:
          limits:
            memory: 500Mi
            cpu: "500m"
          requests:
            memory: 500Mi
            cpu: "500m"
        volumeMounts:
        - name: nvidia-libs
          mountPath: /usr/local/nvidia/lib
          readOnly: true
        env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib:/usr/lib/x86_64-linux-gnu:/lib/x86_64-linux-gnu
        - name: DCGM_EXPORTER_LISTEN
          value: ":9400"
        - name: DCGM_EXPORTER_KUBERNETES
          value: "true"

      - name: local-llm
        image:  harbor.xa.xshixun.com:7443/hanfeigeng/vllm/vllm-openai:v0.13.0-linux-amd64
        #command: ["sleep", "infinity"]
        command: ["bash", "-c"]
        args:
        - |
          # Download model using HF mirror
          echo "Downloading model using HF mirror..."
          HF_ENDPOINT=https://hf-mirror.com huggingface-cli download --resume-download --local-dir-use-symlinks False ByteDance-Seed/Seed-OSS-36B-Instruct
          
          # Start vllm server
          echo "Starting vllm server..."
          vllm serve ByteDance-Seed/Seed-OSS-36B-Instruct --port 8080 --max-model-len 131072 --enable-prefix-caching --enable-chunked-prefill --kv-cache-metrics-sample 1
        ports:
        - containerPort: 8080
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: http_proxy
          value: "http://192.168.3.226:7890"
        - name: https_proxy
          value: "http://192.168.3.226:7890"
        - name: HTTP_PROXY
          value: "http://192.168.3.226:7890"
        - name: HTTPS_PROXY
          value: "http://192.168.3.226:7890"
        - name: APT_PROXY
          value: "http://192.168.3.241:3142"
        - name: PIP_INDEX_URL
          value: "http://192.168.12.70:9181/repository/pypi-proxy/simple/"
        - name: PIP_TRUSTED_HOST
          value: "192.168.12.70"
        - name: HF_ENDPOINT
          value: "https://hf-mirror.com"
        - name: NO_PROXY
          value: "127.0.0.1,localhost,.svc.cluster.local,10.0.0.0/8,192.168.0.0/16,.liuyunxin"
        - name: no_proxy
          value: "127.0.0.1,localhost,.svc.cluster.local,10.0.0.0/8,192.168.0.0/16,.liuyunxin"
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 64Gi
            cpu: "16"
          requests:
            nvidia.com/gpu: 1
            memory: 64Gi
            cpu: "16"
        volumeMounts:
        - name: gpfshome
          mountPath: /root
          # This will automatically make /root/.cache/huggingface persistent
      volumes:
      - name: gpfshome
        persistentVolumeClaim:
          claimName: pvc-gpfshome-liuyunxin
      - name: nvidia-libs
        hostPath:
          path: /usr/lib/x86_64-linux-gnu

---
apiVersion: v1
kind: Service
metadata:
  name: local-llm
  namespace: liuyunxin
spec:
  selector:
    app: local-llm
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP